name: destroy-infra

on:
  workflow_dispatch:
    inputs:
      env:
        description: "Environment (e.g. dev)"
        required: true
        default: "dev"

concurrency:
  group: destroy-${{ github.event.inputs.env }}
  cancel-in-progress: false

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION: ap-southeast-2
  PREFIX: demo-eks
  VPC_STACK: demo-eks-vpc-${{ github.event.inputs.env }}
  EKS_STACK: demo-eks-eks-${{ github.event.inputs.env }}
  NG_STACK:  demo-eks-nodegroup-${{ github.event.inputs.env }}
  ARGO_STACK: demo-eks-argocd-${{ github.event.inputs.env }}

jobs:
  destroy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::685801731369:role/github-oidc-deployer
          aws-region: ${{ env.AWS_REGION }}

      - name: Show current stacks (for context)
        run: |
          aws cloudformation describe-stacks --query "Stacks[].{Name:StackName,Status:StackStatus}" --output table || true

      - name: Derive names & sanity
        id: derive
        run: |
          set -euo pipefail
          echo "cluster_name=${{ env.PREFIX }}-${{ github.event.inputs.env }}" >> $GITHUB_OUTPUT
          echo "argocd_lambda_prefix=${{ env.ARGO_STACK }}-TriggerFn-" >> $GITHUB_OUTPUT
          echo "argocd_cb_logs=/aws/codebuild/argocd-bootstrap-${{ env.PREFIX }}-${{ github.event.inputs.env }}" >> $GITHUB_OUTPUT
          echo "argocd_lambda_logs=/aws/lambda/${{ env.ARGO_STACK }}-TriggerFn" >> $GITHUB_OUTPUT
          echo "account_id=$(aws sts get-caller-identity --query Account --output text)" >> $GITHUB_OUTPUT

      - name: Discover VPC ID from stack (if present)
        id: vpcid
        run: |
          set -euo pipefail
          STACK="${{ env.VPC_STACK }}"
          # Try via CFN resources (works even if the stack is mid-delete)
          VPC_ID=$(aws cloudformation describe-stack-resources --stack-name "$STACK" \
            --query "StackResources[?ResourceType=='AWS::EC2::VPC'].PhysicalResourceId" \
            --output text 2>/dev/null || echo "")
          echo "Found VPC_ID='$VPC_ID'"
          echo "vpc_id=$VPC_ID" >> $GITHUB_OUTPUT

      # --- Optional k8s cleanup (release ELB/PVCs) ---
      # We install kubectl so the step actually works on the runner.
      - name: Install kubectl (for optional cleanup)
        continue-on-error: true
        run: |
          set -euo pipefail
          KVER="1.30.0"
          curl -sLo kubectl "https://dl.k8s.io/release/v${KVER}/bin/linux/amd64/kubectl"
          chmod +x kubectl && sudo mv kubectl /usr/local/bin/

      - name: Attempt Kubernetes cleanup (if accessible)
        continue-on-error: true
        env:
          CLUSTER_NAME: ${{ steps.derive.outputs.cluster_name }}
        run: |
          set -euo pipefail
          if aws eks update-kubeconfig --name "$CLUSTER_NAME" --region $AWS_REGION >/dev/null 2>&1; then
            echo "Kubeconfig set. Deleting LB Services/Ingress/PVCs to release AWS resources..."
            kubectl get svc -A --no-headers | awk '$5 ~ /LoadBalancer/ {print $1, $2}' | xargs -n2 kubectl -n delete svc || true
            kubectl delete ingress --all -A || true
            kubectl delete pvc --all -A || true
          else
            echo "Skipping k8s cleanup (no access)."
          fi

      # --- ArgoCD stack delete + heal ---
      - name: Delete ArgoCD stack (if exists)
        run: |
          if aws cloudformation describe-stacks --stack-name "${{ env.ARGO_STACK }}" >/dev/null 2>&1; then
            aws cloudformation delete-stack --stack-name "${{ env.ARGO_STACK }}"
            aws cloudformation wait stack-delete-complete --stack-name "${{ env.ARGO_STACK }}" || true
          else
            echo "ArgoCD stack not found."
          fi

      - name: Heal ArgoCD DELETE_FAILED (retain blockers)
        if: always()
        run: |
          set -euo pipefail
          STACK="${{ env.ARGO_STACK }}"
          STATUS=$(aws cloudformation describe-stacks --stack-name "$STACK" \
            --query "Stacks[0].StackStatus" --output text 2>/dev/null || echo "NONE")
          if [ "$STATUS" = "DELETE_FAILED" ]; then
            echo "ArgoCD DELETE_FAILEDâ€”retaining failed logical resources, then re-delete..."
            mapfile -t FAILS < <(
              aws cloudformation describe-stack-events --stack-name "$STACK" --max-items 200 \
              --query "reverse(StackEvents)[?contains(ResourceStatus,'DELETE_FAILED')].[LogicalResourceId]" \
              --output text | awk '!seen[$0]++'
            )
            if [[ ${#FAILS[@]} -gt 0 ]]; then
              aws cloudformation delete-stack --stack-name "$STACK" --retain-resources ${FAILS[*]} || true
              aws cloudformation wait stack-delete-complete --stack-name "$STACK" || true
            fi
          fi

      # --- Remove EKS AccessEntry that granted CodeBuild cluster-admin ---
      - name: Remove GitHub OIDC deployer AccessEntry from EKS
        continue-on-error: true
        run: |
          set -euo pipefail
          CLUSTER="${{ steps.derive.outputs.cluster_name }}"
          PRINCIPAL_ARN="arn:aws:iam::685801731369:role/github-oidc-deployer"

          echo "Checking for AccessEntry for $PRINCIPAL_ARN in cluster $CLUSTER..."
          if aws eks describe-access-entry --cluster-name "$CLUSTER" --principal-arn "$PRINCIPAL_ARN" >/dev/null 2>&1; then
            echo "AccessEntry exists. Deleting..."
            aws eks delete-access-entry --cluster-name "$CLUSTER" --principal-arn "$PRINCIPAL_ARN" || true
            echo "Deleted AccessEntry for $PRINCIPAL_ARN"
          else
            echo "No AccessEntry found for $PRINCIPAL_ARN"
          fi

      # --- Nodegroup delete + heal ---
      - name: Delete Nodegroup stack
        run: |
          if aws cloudformation describe-stacks --stack-name "${{ env.NG_STACK }}" >/dev/null 2>&1; then
            aws cloudformation delete-stack --stack-name "${{ env.NG_STACK }}"
            aws cloudformation wait stack-delete-complete --stack-name "${{ env.NG_STACK }}" || true
          else
            echo "Nodegroup stack not found."
          fi

      - name: Heal Nodegroup DELETE_FAILED (retain & clean)
        if: always()
        env:
          CLUSTER_NAME: ${{ steps.derive.outputs.cluster_name }}
        run: |
          set -euo pipefail
          STACK="${{ env.NG_STACK }}"
          STATUS=$(aws cloudformation describe-stacks --stack-name "$STACK" \
            --query "Stacks[0].StackStatus" --output text 2>/dev/null || echo "NONE")
          if [ "$STATUS" = "DELETE_FAILED" ]; then
            echo "Nodegroup DELETE_FAILED: retaining ManagedNodeGroup & NodeRole then re-deleting..."
            aws cloudformation delete-stack --stack-name "$STACK" --retain-resources ManagedNodeGroup NodeRole || true
            aws cloudformation wait stack-delete-complete --stack-name "$STACK" || true
            # Clean orphan nodegroups
            for NG in $(aws eks list-nodegroups --cluster-name "$CLUSTER_NAME" --region $AWS_REGION \
                        --query 'nodegroups[]' --output text 2>/dev/null); do
              aws eks delete-nodegroup --cluster-name "$CLUSTER_NAME" --region $AWS_REGION --nodegroup-name "$NG" || true
              aws eks wait nodegroup-deleted --cluster-name "$CLUSTER_NAME" --region $AWS_REGION --nodegroup-name "$NG" || true
            done
            # Clean the retained role if present
            ROLE=$(aws cloudformation describe-stack-resources --stack-name "$STACK" \
              --logical-resource-id NodeRole --query 'StackResources[0].PhysicalResourceId' --output text 2>/dev/null || echo "")
            if [ -n "$ROLE" ] && aws iam get-role --role-name "$ROLE" >/dev/null 2>&1; then
              for ARN in $(aws iam list-attached-role-policies --role-name "$ROLE" --query 'AttachedPolicies[].PolicyArn' --output text 2>/dev/null); do
                aws iam detach-role-policy --role-name "$ROLE" --policy-arn "$ARN" || true
              done
              for P in $(aws iam list-role-policies --role-name "$ROLE" --query 'PolicyNames[]' --output text 2>/dev/null); do
                aws iam delete-role-policy --role-name "$ROLE" --policy-name "$P" || true
              done
              aws iam delete-role --role-name "$ROLE" || true
            fi
          fi

      # --- Delete IAM OIDC provider tied to the cluster (if it was created outside CFN) ---
      - name: Delete cluster IAM OIDC provider (best-effort)
        continue-on-error: true
        env:
          CLUSTER_NAME: ${{ steps.derive.outputs.cluster_name }}
          ACCOUNT_ID: ${{ steps.derive.outputs.account_id }}
        run: |
          set -euo pipefail
          ISSUER=$(aws eks describe-cluster --name "$CLUSTER_NAME" --region $AWS_REGION \
                   --query 'cluster.identity.oidc.issuer' --output text 2>/dev/null || echo "None")
          if [ "$ISSUER" = "None" ] || [ -z "$ISSUER" ]; then
            echo "Cluster not found or already deleted; skipping OIDC provider cleanup."
            exit 0
          fi
          OIDC_HOST=${ISSUER#https://}
          ARN="arn:aws:iam::${ACCOUNT_ID}:oidc-provider/${OIDC_HOST}"
          if aws iam get-open-id-connect-provider --open-id-connect-provider-arn "$ARN" >/dev/null 2>&1; then
            echo "Deleting IAM OIDC provider $ARN"
            aws iam delete-open-id-connect-provider --open-id-connect-provider-arn "$ARN" || true
          else
            echo "No standalone IAM OIDC provider to delete."
          fi

      # --- EKS control plane delete + heal ---
      - name: Delete EKS stack
        run: |
          if aws cloudformation describe-stacks --stack-name "${{ env.EKS_STACK }}" >/dev/null 2>&1; then
            aws cloudformation delete-stack --stack-name "${{ env.EKS_STACK }}"
            aws cloudformation wait stack-delete-complete --stack-name "${{ env.EKS_STACK }}" || true
          else
            echo "EKS stack not found."
          fi

      - name: Heal EKS DELETE_FAILED (retain EksClusterRole)
        if: always()
        run: |
          set -euo pipefail
          STACK="${{ env.EKS_STACK }}"
          STATUS=$(aws cloudformation describe-stacks --stack-name "$STACK" \
            --query "Stacks[0].StackStatus" --output text 2>/dev/null || echo "NONE")
          if [ "$STATUS" = "DELETE_FAILED" ]; then
            echo "EKS DELETE_FAILED: retaining EksClusterRole then re-deleting..."
            aws cloudformation delete-stack --stack-name "$STACK" --retain-resources EksClusterRole || true
            aws cloudformation wait stack-delete-complete --stack-name "$STACK" || true
            ROLE=$(aws cloudformation describe-stack-resources --stack-name "$STACK" \
              --logical-resource-id EksClusterRole --query 'StackResources[0].PhysicalResourceId' --output text 2>/dev/null || echo "")
            if [ -n "$ROLE" ] && aws iam get-role --role-name "$ROLE" >/dev/null 2>&1; then
              for ARN in $(aws iam list-attached-role-policies --role-name "$ROLE" --query 'AttachedPolicies[].PolicyArn' --output text 2>/dev/null); do
                aws iam detach-role-policy --role-name "$ROLE" --policy-arn "$ARN" || true
              done
              for P in $(aws iam list-role-policies --role-name "$ROLE" --query 'PolicyNames[]' --output text 2>/dev/null); do
                aws iam delete-role-policy --role-name "$ROLE" --policy-name "$P" || true
              done
              aws iam delete-role --role-name "$ROLE" || true
            fi
          fi

      # --- VPC delete + heal ---
      - name: Delete VPC stack
        run: |
          if aws cloudformation describe-stacks --stack-name "${{ env.VPC_STACK }}" >/dev/null 2>&1; then
            aws cloudformation delete-stack --stack-name "${{ env.VPC_STACK }}"
            aws cloudformation wait stack-delete-complete --stack-name "${{ env.VPC_STACK }}" || true
          else
            echo "VPC stack not found."
          fi

      - name: Heal VPC DELETE_FAILED (retain blockers to clear stack record)
        if: always()
        run: |
          set -euo pipefail
          STACK="${{ env.VPC_STACK }}"
          STATUS=$(aws cloudformation describe-stacks --stack-name "$STACK" \
            --query "Stacks[0].StackStatus" --output text 2>/dev/null || echo "NONE")
          if [ "$STATUS" = "DELETE_FAILED" ]; then
            echo "VPC DELETE_FAILEDâ€”retaining blockers so the stack record can be removed."
            mapfile -t FAILS < <(
              aws cloudformation describe-stack-events --stack-name "$STACK" --max-items 200 \
              --query "reverse(StackEvents)[?contains(ResourceStatus,'DELETE_FAILED')].[LogicalResourceId]" \
              --output text | awk '!seen[$0]++'
            )
            if [[ ${#FAILS[@]} -gt 0 ]]; then
              aws cloudformation delete-stack --stack-name "$STACK" --retain-resources ${FAILS[*]} || true
              aws cloudformation wait stack-delete-complete --stack-name "$STACK" || true
              echo "NOTE: Retained resources remain in your account (VPC, subnets, etc.)."
            fi
          fi

      - name: Deep clean VPC dependencies (best-effort)
        if: always()
        continue-on-error: true
        env:
          VPC_ID: ${{ steps.vpcid.outputs.vpc_id }}
        run: |
          set -euo pipefail
          if [ -z "${VPC_ID:-}" ]; then
            echo "No VPC_ID captured earlier; skipping deep clean."
            exit 0
          fi
          echo "Deep cleaning VPC: $VPC_ID in $AWS_REGION"

          # --- Detach & delete Internet Gateways ---
          for IGW in $(aws ec2 describe-internet-gateways --region "$AWS_REGION" \
                    --filters "Name=attachment.vpc-id,Values=$VPC_ID" \
                    --query "InternetGateways[].InternetGatewayId" --output text); do
            aws ec2 detach-internet-gateway --internet-gateway-id "$IGW" --vpc-id "$VPC_ID" --region "$AWS_REGION" || true
            aws ec2 delete-internet-gateway --internet-gateway-id "$IGW" --region "$AWS_REGION" || true
          done

          # --- Delete VPC endpoints ---
          EPS=$(aws ec2 describe-vpc-endpoints --region "$AWS_REGION" \
                --filters "Name=vpc-id,Values=$VPC_ID" \
                --query "VpcEndpoints[].VpcEndpointId" --output text)
          if [ -n "$EPS" ]; then
            aws ec2 delete-vpc-endpoints --vpc-endpoint-ids $EPS --region "$AWS_REGION" || true
          fi

          # --- Delete NAT gateways (wait until deleted) ---
          for NGW in $(aws ec2 describe-nat-gateways --region "$AWS_REGION" \
                    --filter Name=vpc-id,Values=$VPC_ID \
                    --query "NatGateways[].NatGatewayId" --output text); do
            aws ec2 delete-nat-gateway --nat-gateway-id "$NGW" --region "$AWS_REGION" || true
          done

          # --- Delete non-main route tables + their associations ---
          for RT in $(aws ec2 describe-route-tables --region "$AWS_REGION" \
                    --filters "Name=vpc-id,Values=$VPC_ID" \
                    --query "RouteTables[].{Id:RouteTableId,Main:Associations[?Main].Main | [0]} | [?Main!=\`true\`].Id" \
                    --output text); do
            for A in $(aws ec2 describe-route-tables --region "$AWS_REGION" \
                      --route-table-ids "$RT" \
                      --query "RouteTables[0].Associations[?AssociationState.State=='associated'].RouteTableAssociationId" \
                      --output text); do
              aws ec2 disassociate-route-table --association-id "$A" --region "$AWS_REGION" || true
            done
            # remove extra routes (skip the local one)
            for R in $(aws ec2 describe-route-tables --region "$AWS_REGION" --route-table-ids "$RT" \
                      --query "RouteTables[0].Routes[?Origin!='CreateRouteTable'].DestinationCidrBlock" \
                      --output text); do
              aws ec2 delete-route --route-table-id "$RT" --destination-cidr-block "$R" --region "$AWS_REGION" || true
            done
            aws ec2 delete-route-table --route-table-id "$RT" --region "$AWS_REGION" || true
          done

          # --- Delete custom NACLs (leave default) ---
          for NACL in $(aws ec2 describe-network-acls --region "$AWS_REGION" \
                     --filters "Name=vpc-id,Values=$VPC_ID" \
                     --query "NetworkAcls[?IsDefault==\`false\`].NetworkAclId" --output text); do
            aws ec2 delete-network-acl --network-acl-id "$NACL" --region "$AWS_REGION" || true
          done

          # --- Delete non-default security groups ---
          for SG in $(aws ec2 describe-security-groups --region "$AWS_REGION" \
                   --filters "Name=vpc-id,Values=$VPC_ID" \
                   --query "SecurityGroups[?GroupName!='default'].GroupId" --output text); do
            # remove any dangling rules referencing other SGs
            aws ec2 revoke-security-group-egress  --group-id "$SG" --ip-permissions "$(aws ec2 describe-security-groups --group-ids "$SG" --query 'SecurityGroups[0].IpPermissionsEgress')" --region "$AWS_REGION" 2>/dev/null || true
            aws ec2 revoke-security-group-ingress --group-id "$SG" --ip-permissions "$(aws ec2 describe-security-groups --group-ids "$SG" --query 'SecurityGroups[0].IpPermissions')"        --region "$AWS_REGION" 2>/dev/null || true
            aws ec2 delete-security-group --group-id "$SG" --region "$AWS_REGION" || true
          done

          # --- Delete ENIs in 'available' state ---
          for ENI in $(aws ec2 describe-network-interfaces --region "$AWS_REGION" \
                    --filters "Name=vpc-id,Values=$VPC_ID" "Name=status,Values=available" \
                    --query "NetworkInterfaces[].NetworkInterfaceId" --output text); do
            aws ec2 delete-network-interface --network-interface-id "$ENI" --region "$AWS_REGION" || true
          done

          # --- Delete subnets ---
          for SUB in $(aws ec2 describe-subnets --region "$AWS_REGION" \
                    --filters "Name=vpc-id,Values=$VPC_ID" \
                    --query "Subnets[].SubnetId" --output text); do
            aws ec2 delete-subnet --subnet-id "$SUB" --region "$AWS_REGION" || true
          done

          # --- Delete VPC Flow Logs ---
          for FL in $(aws ec2 describe-flow-logs --region "$AWS_REGION" \
                    --filter Name=resource-id,Values=$VPC_ID \
                    --query "FlowLogs[].FlowLogId" --output text); do
            aws ec2 delete-flow-logs --flow-log-ids $FL --region "$AWS_REGION" || true
          done

          # --- Disassociate custom DHCP options (set default) ---
          DEFAULT_DHCP=$(aws ec2 describe-dhcp-options --region "$AWS_REGION" \
                         --filters "Name=default,Values=true" \
                         --query "DhcpOptions[0].DhcpOptionsId" --output text 2>/dev/null || echo "")
          if [ -n "$DEFAULT_DHCP" ] && [ "$DEFAULT_DHCP" != "None" ]; then
            aws ec2 associate-dhcp-options --region "$AWS_REGION" --dhcp-options-id "$DEFAULT_DHCP" --vpc-id "$VPC_ID" || true
          fi

          # --- Finally, delete the VPC ---
          aws ec2 delete-vpc --vpc-id "$VPC_ID" --region "$AWS_REGION" || true
          echo "Deep clean complete (best-effort)."          
      # --- Orphans cleanup (logs, CodeBuild/Lambda, SSM params, ELBs, ENIs, EBS) ---
      - name: Delete CloudWatch log groups (Lambda TriggerFn & CodeBuild)
        continue-on-error: true
        run: |
          set -euo pipefail
          REG="${AWS_REGION}"
          for LG in $(aws logs describe-log-groups --region "$REG" \
              --log-group-name-prefix "${{ steps.derive.outputs.argocd_lambda_logs }}" \
              --query "logGroups[].logGroupName" --output text 2>/dev/null); do
            echo "Deleting $LG"
            aws logs delete-log-group --region "$REG" --log-group-name "$LG" || true
          done
          CB_LG="${{ steps.derive.outputs.argocd_cb_logs }}"
          aws logs delete-log-group --region "$REG" --log-group-name "$CB_LG" || true

      - name: Delete CodeBuild project & Lambda functions (if any)
        continue-on-error: true
        run: |
          set -euo pipefail
          REG="${AWS_REGION}"
          CLUSTER="${{ steps.derive.outputs.cluster_name }}"
          PROJ="argocd-bootstrap-${CLUSTER}"
          if aws codebuild batch-get-projects --names "$PROJ" --region "$REG" >/dev/null 2>&1; then
            echo "Deleting CodeBuild project $PROJ"
            aws codebuild delete-project --name "$PROJ" --region "$REG" || true
          fi
          for FN in $(aws lambda list-functions --region "$REG" \
              --query "Functions[?starts_with(FunctionName, '${{ env.ARGO_STACK }}-TriggerFn-')].FunctionName" \
              --output text 2>/dev/null); do
            echo "Deleting Lambda $FN"
            aws lambda delete-function --function-name "$FN" --region "$REG" || true
          done

      - name: Cleanup SSM params under /eks/<env> and /eks/<cluster>
        continue-on-error: true
        run: |
          set -euo pipefail
          REG="${AWS_REGION}"
          ENV_PATH="/eks/${{ github.event.inputs.env }}"
          CLUSTER_PATH="/eks/${{ steps.derive.outputs.cluster_name }}"
          for PFX in "$ENV_PATH" "$CLUSTER_PATH"; do
            echo "Deleting SSM parameters under $PFX ..."
            for name in $(aws ssm get-parameters-by-path --region "$REG" --path "$PFX" --recursive --query 'Parameters[].Name' --output text 2>/dev/null); do
              echo "Deleting $name"
              aws ssm delete-parameter --region "$REG" --name "$name" || true
            done
          done

      - name: Nuke stray ELBv2 / Classic ELB with cluster tag (best-effort)
        continue-on-error: true
        run: |
          set -euo pipefail
          REG="${AWS_REGION}"
          CLUSTER="${{ steps.derive.outputs.cluster_name }}"
          # ELBv2 (ALB/NLB)
          for ARN in $(aws elbv2 describe-load-balancers --region "$REG" \
                --query "LoadBalancers[].LoadBalancerArn" --output text 2>/dev/null); do
            TAG=$(aws elbv2 describe-tags --resource-arns $ARN --region "$REG" \
                  --query "TagDescriptions[0].Tags[?Key=='kubernetes.io/cluster/${CLUSTER}'].Value | [0]" --output text 2>/dev/null || echo "")
            if [ "$TAG" = "owned" ] || [ "$TAG" = "shared" ]; then
              echo "Deleting ELBv2 $ARN"
              aws elbv2 delete-load-balancer --load-balancer-arn "$ARN" --region "$REG" || true
            fi
          done
          # Classic ELB
          for NAME in $(aws elb describe-load-balancers --region "$REG" \
                --query "LoadBalancerDescriptions[].LoadBalancerName" --output text 2>/dev/null); do
            TAG=$(aws elb describe-tags --load-balancer-names "$NAME" --region "$REG" \
                  --query "TagDescriptions[0].Tags[?Key=='kubernetes.io/cluster/${CLUSTER}'].Value | [0]" --output text 2>/dev/null || echo "")
            if [ "$TAG" = "owned" ] || [ "$TAG" = "shared" ]; then
              echo "Deleting classic ELB $NAME"
              aws elb delete-load-balancer --load-balancer-name "$NAME" --region "$REG" || true
            fi
          done

      - name: Nuke stray ENIs & EBS tagged for the cluster (best-effort)
        continue-on-error: true
        run: |
          set -euo pipefail
          REG="${AWS_REGION}"
          CLUSTER="${{ steps.derive.outputs.cluster_name }}"
          # ENIs (detached only)
          for ENI in $(aws ec2 describe-network-interfaces --region "$REG" \
                --filters "Name=tag:kubernetes.io/cluster/${CLUSTER},Values=owned,shared" \
                          "Name=status,Values=available" \
                --query "NetworkInterfaces[].NetworkInterfaceId" --output text 2>/dev/null); do
            echo "Deleting ENI $ENI"
            aws ec2 delete-network-interface --network-interface-id "$ENI" --region "$REG" || true
          done
          # EBS
          for VOL in $(aws ec2 describe-volumes --region "$REG" \
                --filters "Name=tag:kubernetes.io/cluster/${CLUSTER},Values=owned,shared" \
                --query "Volumes[].VolumeId" --output text 2>/dev/null); do
            STATE=$(aws ec2 describe-volumes --region "$REG" --volume-ids "$VOL" --query "Volumes[0].State" --output text)
            if [ "$STATE" = "available" ]; then
              echo "Deleting EBS volume $VOL"
              aws ec2 delete-volume --volume-id "$VOL" --region "$REG" || true
            else
              echo "Skipping EBS $VOL (state=$STATE)"
            fi
          done

      - name: Final stack states
        run: |
          aws cloudformation describe-stacks --query "Stacks[].{Name:StackName,Status:StackStatus}" --output table || true